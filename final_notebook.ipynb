{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd92621d-793a-449d-b74b-1479fb33a7d6",
   "metadata": {},
   "source": [
    "# Git Shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b574e81-8fcd-4015-b368-69a3b3488264",
   "metadata": {},
   "source": [
    "## Project Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36cf5b-d496-4718-add5-d3799b458e64",
   "metadata": {},
   "source": [
    "- Determine the programming language used in a given GitHub Repository by examining the words used in the readme file.\n",
    "\n",
    "- We honed in specifically on repositories with the word 'shoes' in the name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91a5c2-b027-4252-a6eb-f6fee66a11eb",
   "metadata": {},
   "source": [
    "## Imports Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbb7bba-c3ef-45e3-b3f7-564443b4b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import unicodedata\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, recall_score\n",
    "\n",
    "from time import strftime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from env import get_connection\n",
    "import acquire as ac\n",
    "import prepare as prep\n",
    "import explore_functions as ex\n",
    "import modeling as md\n",
    "from acquire import scrape_github_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a14ded-0b02-4480-90ff-e61ce9c51896",
   "metadata": {},
   "source": [
    "# Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ad43a-ba19-4daf-a75a-c052768e0acc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "- We obtained our data through the GitHub api, through the use of many functions within the acquire.py file.\n",
    "\n",
    "- We started with 198 rows in our data, with each row being an individual repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bad9ba-fa82-496f-bcba-da5dfaffa293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>justinmk/vim-sneak</td>\n",
       "      <td>Vim Script</td>\n",
       "      <td>sneak.vim ðŸ‘Ÿ\\n================\\n\\nJump to any l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google-research-datasets/Objectron</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>\\n&lt;div align=\"center\"&gt;\\n\\n# Objectron Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shoes/shoes4</td>\n",
       "      <td>Ruby</td>\n",
       "      <td># Shoes 4 [![Linux Build Status](https://secur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shoes/shoes-deprecated</td>\n",
       "      <td>C</td>\n",
       "      <td># THIS REPO IS NO LONGER ACTIVE!\\n\\n**Looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filamentgroup/shoestring</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>:warning: This project is archived and the rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 repo          language  \\\n",
       "0                  justinmk/vim-sneak        Vim Script   \n",
       "1  google-research-datasets/Objectron  Jupyter Notebook   \n",
       "2                        shoes/shoes4              Ruby   \n",
       "3              shoes/shoes-deprecated                 C   \n",
       "4            filamentgroup/shoestring        JavaScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  sneak.vim ðŸ‘Ÿ\\n================\\n\\nJump to any l...  \n",
       "1  \\n<div align=\"center\">\\n\\n# Objectron Dataset\\...  \n",
       "2  # Shoes 4 [![Linux Build Status](https://secur...  \n",
       "3  # THIS REPO IS NO LONGER ACTIVE!\\n\\n**Looking ...  \n",
       "4  :warning: This project is archived and the rep...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling the data from the json file\n",
    "df = pd.read_json('repos.json')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7530d6c-b451-4e17-b7e1-d0afac5d051f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470df088-ac1c-4335-81be-90b7687fa921",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In order to make sense of our data, we had to clean it up quite a bit. We started by doing:\n",
    "\n",
    "- There were two repositories that had nothing to do with out chosen topic, shoes, and so we dropped those two.\n",
    "    \n",
    "- There were also 13 rows that had no readme files, and thus those were also dropped.\n",
    "    \n",
    "- Next, we got a value count of the various languages we had, and anything that was beloew 10 we consolidated into one category, 'other'\n",
    "    \n",
    "- After that, we created a series for each unique language as well as got a value count for each\n",
    "    \n",
    "- After creating those other variables for later exploration, we cleaned up our readme entries by normalizing the words to removing accents and other symbols that arent helpful, as well as removing stopwords and making each word its own token to make it easier to work with later.\n",
    "    \n",
    "- Lastly, because we were working with a reasonable amount of data, we both stemmed and lemmatized so that we could determine what works better on our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e8240-6d7c-443e-8538-4dc9fa96eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \n",
    "    df = df.drop([29, 139])\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    others = ['TypeScript', 'Jupyter Notebook', 'Java', 'C#', 'Swift', 'CSS', 'C', 'C++', 'Kotlin',\n",
    "         'VimL', 'Handlebars', 'Vue', 'Go', 'SCSS', 'Emacs Lisp', 'Vim Script', 'Lua', 'TeX',\n",
    "         'Rust', 'Shell', 'PHP', 'Vim script', 'CoffeeScript']\n",
    "\n",
    "    df = df.replace(to_replace=others, value=\"Other\")\n",
    "\n",
    "    df['language'] = df['language'].str.lower()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc84c0f2-4b73-4057-976b-01d37fe8e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep.clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee794ed-08f9-4f07-b303-217f95d26481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(df):\n",
    "    \n",
    "    other_words = prep.clean_text(' '.join(df[df['language'] == 'Other']['readme_contents']))\n",
    "    javascript_words = prep.clean_text(' '.join(df[df['language'] == 'JavaScript']['readme_contents']))\n",
    "    html_words = prep.clean_text(' '.join(df[df['language'] == 'HTML']['readme_contents']))\n",
    "    dart_words = prep.clean_text(' '.join(df[df['language'] == 'Dart']['readme_contents']))\n",
    "    ruby_words = prep.clean_text(' '.join(df[df['language'] == 'Ruby']['readme_contents']))\n",
    "    python_words = prep.clean_text(' '.join(df[df['language'] == 'Python']['readme_contents']))\n",
    "    all_words = prep.clean_text(' '.join(df['readme_contents']))\n",
    "\n",
    "    other_counts = pd.Series(other_words).value_counts()\n",
    "    javascript_counts = pd.Series(javascript_words).value_counts()\n",
    "    html_counts = pd.Series(html_words).value_counts()\n",
    "    dart_counts = pd.Series(dart_words).value_counts()\n",
    "    ruby_counts = pd.Series(ruby_words).value_counts()\n",
    "    python_counts = pd.Series(python_words).value_counts()\n",
    "    all_counts = pd.Series(all_words).value_counts()\n",
    "\n",
    "    \n",
    "    word_freq = pd.concat([other_counts, javascript_counts, html_counts, dart_counts, \n",
    "                       ruby_counts, python_counts, all_counts], axis=1)\n",
    "\n",
    "    word_freq.fillna(0, inplace=True)\n",
    "    \n",
    "    word_freq = word_freq.astype('int')\n",
    "\n",
    "    word_freq = word_freq.rename(columns={0:'other', 1:'javascript', 2:'html', 3: 'dart', 4:'ruby', 5:'python', 6:'all_counts'})\n",
    "\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61f9cf38-831b-4407-925b-93714884db96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other</th>\n",
       "      <th>javascript</th>\n",
       "      <th>html</th>\n",
       "      <th>dart</th>\n",
       "      <th>ruby</th>\n",
       "      <th>python</th>\n",
       "      <th>all_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>install</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         other  javascript  html  dart  ruby  python  all_counts\n",
       "product      0           0     0     0     0       0         279\n",
       "user         0           0     0     0     0       0         260\n",
       "file         0           0     0     0     0       0         224\n",
       "page         0           0     0     0     0       0         209\n",
       "install      0           0     0     0     0       0         207"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.word_counts(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fff4aca-c2f6-4ac8-82e0-bc6a3dbaaaf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prep_article_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zk/vf208zwx67vf8nf5b5xtyllw0000gn/T/ipykernel_52153/2934518169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprep_article_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'readme_contents'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prep_article_data' is not defined"
     ]
    }
   ],
   "source": [
    "prep.prep_article_data(df, 'readme_contents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7d851-4bfa-466a-b360-56b6b5e6332f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c3c3b-cd53-4880-abf5-bf886cfca08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eee87b7b-31ca-4e5b-a407-dc8c3af0aaf7",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72898fb2-2c62-43e7-9f44-c63d646fbb5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ab2da-c52f-4512-91f1-31fe819f5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_train_test(df)\n",
    "\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd0844-7d27-4618-a933-65409b3df349",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = xy_train(train, validate, test, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386fc7e-450a-4990-8e36-306af47a5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450387a8-2128-4235-bcd0-6f3b9f13ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our baseline accuracy\n",
    "df['baseline'] = df['language'].value_counts().idxmax()\n",
    "\n",
    "print((df['language'] == df['baseline']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc98c8-e2d8-4b22-9a8b-5c994c80a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.random_forest(X_train['stemmed'], y_train, X_validate['stemmed'], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39416da5-054b-44dc-9bf8-1bd993ba79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.random_forest(X_train['lemmatized'], y_train, X_validate['lemmatized'], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62aa8fc-8657-40dd-8e75-5c53cc01f0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29585825-0769-41e9-8dc9-9322614c064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.dec_tree(X_train['stemmed'], y_train, X_validate['stemmed'], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49468ba-dcad-44e5-b54c-372873f3a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.dec_tree(X_train['lemmatized'], y_train, X_validate['lemmatized'], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c961c-1de9-472c-9ed1-392edd53ed50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c239c07-f359-484e-9e7c-f4d4fb99c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.knn(X_train['stemmed'], y_train, X_validate['stemmed'], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f45b3e-befb-4385-88e2-703f861f6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.knn(X_train['lemmatized'], y_train, X_validate['lemmatized'], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29a734-7fe4-4e09-a3d9-7d8182ef1fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435b295-5328-49b5-b437-c389244bed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(X_train['stemmed'], y_train, X_validate['stemmed'], y_validate, X_test['stemmed'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebee2a-f7d4-4ca7-b689-e7e500ecc3eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b739004-cb7c-45c9-822c-49efc40a87b7",
   "metadata": {},
   "source": [
    "## Modeling Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f869f-556c-4550-95bf-2262a45aa9a6",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a770c-46df-4f50-9d50-6f01a93d29c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99be9aa7-f053-4513-be25-d22c11b3ca1f",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfec7c8-99c2-4d0f-8321-809c62be3535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c1ebe5-8535-40dc-a30c-2095fccfe2ee",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f121b-760f-4449-a2b9-ab598a61f75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
